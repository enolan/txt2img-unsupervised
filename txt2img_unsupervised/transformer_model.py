import flash_attention_jax # Pure JAX flash attention implementation by lucidrains
import flash_attn_jax as flash_attention_cpp # C++ flash attention by Tri Dao et al, JAX bindings by nshepperd
import flax.core
import flax.linen as nn
import jax
import jax.numpy as jnp
import numpy as np
import optax  # type: ignore[import]
import pytest
from copy import copy
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from einops import rearrange, repeat
from flax import struct
from functools import partial
from infinidata import TableView
from pathlib import Path
from typing import Any, Callable, Optional, Tuple
from tqdm import tqdm, trange

from .cone_sampling import random_pt_with_cosine_similarity
from .config import ModelConfig
from .gen_training_caps import gen_training_examples_from_tree
from .load_pq_dir import load_pq_to_infinidata
from .spherical_space_partitioning import CapTree
from .triangle_schedule import triangle_schedule


AttnMethod = Enum('AttnMethod', ['STANDARD', 'FLASH_JAX', 'FLASH_CPP'])


class ImageModel(nn.Module):
    """A transformer model for images encoded to a discrete representation."""

    d_model: int
    num_heads: int
    ff_dim: int
    dropout: Optional[float]
    n_layers: int
    image_tokens: int
    clip_conditioning: bool
    clip_caps: bool
    clip_cap_count: Optional[int]
    use_biases: bool
    activations_dtype: jnp.dtype
    activation_function: Callable[[jax.Array], jax.Array]
    pre_norm: bool
    decode: bool = False
    attn_method: AttnMethod = AttnMethod.FLASH_JAX

    def setup(self) -> None:
        default_stddev = 0.02 / jnp.sqrt(self.n_layers)
        default_kernel_init = nn.initializers.normal(stddev=default_stddev)
        self.in_embed = nn.Embed(
            num_embeddings=8192,
            features=self.d_model,
            embedding_init=default_kernel_init,
            dtype=self.activations_dtype,
        )
        # A note on how CLIP conditioning works:
        # There are three modes:
        # 1) No conditioning. We prepend a zero token to the input sequence.

        # 2) Conditioning on one CLIP embedding. We project the CLIP embedding into d_model
        # and prepend it to the input sequence. This works, and the model learns to produce images
        # that look like an image with that CLIP embedding.

        # 3) Conditioning on one or more spherical caps. CLIP embeddings are unit vectors, and the
        # image's embedding must fall inside all the caps. We project the cap's center and the
        # maximum cosine distance the embedding can have from the center into d_model, and then
        # sum them, producing one conditioning token per cap, which we then prepend to the input.
        # So the image's embedding must be in the intersection of all the caps. I'm trying this
        # for a few reasons: firstly, because an ideal model conditioning on a single exact
        # embedding would learn to produce images with that exact embedding, when in reality
        # image-text prompt cosine similarities are usually between like 0.15 and 0.4 and
        # image-very similiar image cosine similarities are around 0.8. Secondly, because the
        # intersections may allow you to do things with prompting that are impossible with a single
        # prompt. Other models do allow multiple prompts but AFAICT they're not doing
        # *intersections* they're just taking an average. Thirdly, this gives us something similar
        # to classifier free guidance - prompting with bigger caps is like lower cfg scale and
        # will hopefully lead to more realistic - though less prompt-aligned - images.
        # N.B. the more caps we condition on the more the model learns about the CLIP embeddings
        # of the examples at train time. One cap may be a very large area, but more allows it to
        # effectively "triangulate".

        # At training time we use precomputed cap-image pairs, generated by gen_training_caps.py

        # 1 & 2 work, we'll find out whether 3 does soon, hopefully.

        assert (
            self.clip_conditioning or not self.clip_caps
        ), "Can't use clip_caps without clip_conditioning"

        if self.clip_caps:
            assert self.clip_cap_count is not None, "clip_cap_count must be set"
            assert self.clip_cap_count > 0, "clip_cap_count must be positive"

        # The initializers for CLIP conditioning are chosen such that the projected clip emedding
        # has the same distribution as the token embeddings and the projected max cosine distances
        # have the same average magnitude as the token embeddings, assuming the distances are drawn
        # from U[0, 2]. The max distances need to be converted into min similarities for the math
        # to work.
        self.clip_proj = nn.Dense(
            features=self.d_model,
            use_bias=self.use_biases,
            dtype=self.activations_dtype,
            kernel_init=default_kernel_init,
        )
        self.max_cos_distance_proj = nn.Dense(
            features=self.d_model,
            use_bias=self.use_biases,
            dtype=self.activations_dtype,
            kernel_init=nn.initializers.normal(stddev=2 * default_stddev),
        )
        self.positional_encoding = nn.Embed(
            num_embeddings=self.seq_len(),
            features=self.d_model,
            embedding_init=default_kernel_init,
            dtype=self.activations_dtype,
        )
        # it'd potentially be better to use nn.remat_scan here, but it makes inference massively
        # slower for some reason. Even though checkpointing should only affect gradient computation.
        # Might have to do with the fact that remat_scan creates a scan-of-scans? Could cause bad
        # optimization in JAX or XLA.
        self.transformer_layers = nn.scan(
            nn.remat(TransformerLayer),
            variable_axes={"params": 0, "cache": 0},
            variable_broadcast=False,
            split_rngs={"params": True, "dropout": True},
            length=self.n_layers,
        )(
            d_model=self.d_model,
            num_heads=self.num_heads,
            ff_dim=self.ff_dim,
            dropout=self.dropout,
            use_biases=self.use_biases,
            activations_dtype=self.activations_dtype,
            activation_function=self.activation_function,
            pre_norm=self.pre_norm,
            kernel_init=default_kernel_init,
            decode=self.decode,
            attn_method=self.attn_method,
        )

        if self.pre_norm:
            self.final_layer_norm = nn.LayerNorm(dtype=self.activations_dtype)

        self.logits_decoder = nn.Dense(
            features=8192,
            kernel_init=default_kernel_init,
            use_bias=self.use_biases,
            dtype=self.activations_dtype,
        )

        tokens_res = int(self.image_tokens**0.5)
        assert tokens_res * tokens_res == self.image_tokens

    def seq_len(self) -> int:
        """How many tokens are in the sequence being modeled."""
        return self.image_tokens + self.prepended_tokens() - 1

    def prepended_tokens(self) -> int:
        """How many tokens are prepended to the image tokens."""
        # We always prepend at least one, becuase the first image token must be conditioned on
        # *something* even when that's a constant.
        if self.clip_conditioning:
            if self.clip_caps:
                return self.clip_cap_count
            else:
                return 1
        else:
            return 1

    def gen_conditioning_tokens(
        self,
        clip_embedding: jax.Array,
        max_cos_distance: jax.Array,
    ) -> jax.Array:
        """Generate the conditioning tokens that should be prepended to the image tokens. Returns a
        (self.prepended_tokens(), self.d_model) shaped array."""
        if not self.clip_conditioning:
            assert clip_embedding.shape == max_cos_distance.shape == (0,)
            res = jnp.zeros((1, self.d_model), dtype=self.activations_dtype)
        else:
            if not self.clip_caps:
                assert clip_embedding.shape == (768,)
                assert max_cos_distance.shape == (0,)
                res = self.clip_proj(clip_embedding)[None, :]
            else:
                assert clip_embedding.shape == (self.clip_cap_count, 768)
                assert max_cos_distance.shape == (self.clip_cap_count,)

                # Without this rearrange when we apply the dense layer it interprets the max cos
                # distances as a single vector of length n rather than n vectors of length 1, and
                # produces one embedding for the whole sequence of distances rather than one for
                # each cap. Shapes, man.
                max_cos_distance = rearrange(max_cos_distance, "caps -> caps 1")

                res_cap_centers = self.clip_proj(clip_embedding)
                res_max_cos_distances = self.max_cos_distance_proj(1 - max_cos_distance)
                assert res_cap_centers.shape == (self.clip_cap_count, self.d_model)
                assert res_max_cos_distances.shape == (
                    self.clip_cap_count,
                    self.d_model,
                )
                res = (res_cap_centers + res_max_cos_distances) / 2
        assert res.shape == (self.prepended_tokens(), self.d_model)
        return res

    def output_shape_tokens(self) -> int:
        """What (2-D) shape of tokens is output by the model."""
        res = int(self.image_tokens**0.5)
        return (res, res)

    def __call__(
        self,
        images: jax.Array,
        clip_embeddings: jax.Array,
        max_cos_distances: jax.Array,
    ) -> jax.Array:
        """Run the model, returning log probabilities of the image tokens. No probabilities are computed
        for any CLIP conditioning tokens."""
        assert_msg = f"Expected images array with shape (N, {self.image_tokens}), got {images.shape}"
        assert len(images.shape) == 2, assert_msg
        assert images.shape[1] == self.image_tokens, assert_msg
        assert images.dtype == jnp.int32 or images.dtype == jnp.int64

        batch_size = images.shape[0]

        if self.clip_conditioning:
            if self.clip_caps:
                assert clip_embeddings.shape == (batch_size, self.clip_cap_count, 768)
                assert max_cos_distances.shape == (batch_size, self.clip_cap_count)
            else:
                assert clip_embeddings.shape == (batch_size, 768)
                assert max_cos_distances.shape == (batch_size, 0)
        else:
            assert clip_embeddings.shape == max_cos_distances.shape == (batch_size, 0)

        embeds = self.in_embed(images)
        assert embeds.shape == (batch_size, self.image_tokens, self.d_model)

        cond_tokens = jax.vmap(self.gen_conditioning_tokens)(
            clip_embeddings, max_cos_distances
        )
        assert cond_tokens.shape == (batch_size, self.prepended_tokens(), self.d_model)
        toks = jnp.concatenate([cond_tokens, embeds[:, :-1]], axis=1)
        assert toks.shape == (batch_size, self.seq_len(), self.d_model)

        h: jax.Array = toks + self.positional_encoding(jnp.arange(self.seq_len()))
        assert h.shape == (batch_size, self.seq_len(), self.d_model)
        h, _ = self.transformer_layers(h, None)
        assert h.shape == (batch_size, self.seq_len(), self.d_model)
        if self.pre_norm:
            h = self.final_layer_norm(h)
        h = h[:, self.prepended_tokens() - 1 :]
        h = self.logits_decoder(h)
        assert h.shape == (batch_size, self.image_tokens, 8192)
        assert h.dtype == self.activations_dtype

        return h

    def decode_init(
        self,
        clip_embedding: jax.Array,
        max_cos_distance: jax.Array,
    ):
        """Initialize the cache for decoding by computing and feeding the conditioning tokens. Returns
        the logits for the first image token. The cache should be ready for use with decode_step
        when this is done. Batchless (for now)."""
        assert self.decode
        # TODO test CPP flash attention, maybe it works.
        assert self.attn_method == AttnMethod.STANDARD, "Only standard attention works with decoding."

        if self.clip_conditioning:
            if self.clip_caps:
                assert clip_embedding.shape == (self.clip_cap_count, 768)
                assert max_cos_distance.shape == (self.clip_cap_count,)
            else:
                assert clip_embedding.shape == (768,)
                assert max_cos_distance.shape == (0,)
        else:
            assert (
                clip_embedding.shape == max_cos_distance.shape == (0,)
            ), f"Expected empty shapes, got {clip_embedding.shape} and {max_cos_distance.shape}"

        cond_tokens = self.gen_conditioning_tokens(clip_embedding, max_cos_distance)

        h = cond_tokens + self.positional_encoding(jnp.arange(self.prepended_tokens()))
        assert h.shape == (self.prepended_tokens(), self.d_model)

        # TODO vectorize, don't loop
        for tok in h:
            # Add batch dimension and sequence dimension before running transformer blocks
            h, _ = self.transformer_layers(tok[None, None, :], None)
            last_tok = h[0, 0]
        assert last_tok.shape == (self.d_model,)

        logits_out = self.logits_decoder(last_tok)
        assert logits_out.shape == (8192,)
        return logits_out

    def decode_step(self, tok: jax.Array, idx: jax.Array) -> jax.Array:
        """Do a step of iterative decoding from the model. Returns the logits for the next token.
        See below tests for usage examples. Batchless (for now).
        """
        assert (
            self.decode
        ), "Can't call decode_step on a model that wasn't set up for decoding."
        assert self.attn_method == AttnMethod.STANDARD, "Only standard attention works with decoding."
        assert tok.shape == ()
        assert tok.dtype == jnp.int32 or tok.dtype == jnp.int64
        assert idx.shape == ()

        embed = self.in_embed(tok)
        assert embed.shape == (self.d_model,)

        h = embed + self.positional_encoding(idx + self.prepended_tokens())
        assert h.shape == (self.d_model,)

        h, _ = self.transformer_layers(h[None, None, :], None)
        return self.logits_decoder(h[0, 0])  # type: ignore[no-any-return]


def _assert_dicts_equal(d1, d2, name) -> None:
    assert isinstance(d1, dict)
    assert isinstance(d2, dict)
    assert d1.keys() == d2.keys()
    for k in d1.keys():
        if isinstance(d1[k], dict):
            _assert_dicts_equal(d1[k], d2[k], f"{name}.{k}")
        elif isinstance(d1[k], jax.Array):
            np.testing.assert_allclose(
                np.array(d1[k]), np.array(d2[k]), atol=1e-8, rtol=0
            )
        else:
            assert False, f"unknown type {type(d1[k])} for {name}.{k}"


def _setup_test_sample(
    clip_conditioning: bool = False,
    clip_caps: bool = False,
    clip_cap_count: Optional[int] = None,
    image_tokens: int = 256,
) -> Tuple[ImageModel, ImageModel, dict, jax.Array, jax.Array]:
    """Shared setup code for iterative sampling tests."""
    cfg_nodec = copy(gpt_1_config)
    cfg_nodec.dropout = None
    cfg_nodec.image_tokens = image_tokens
    # smaller model makes debug output easier to read
    cfg_nodec.n_layers = 2
    cfg_nodec.d_model = 64
    cfg_nodec.num_heads = 4
    if clip_conditioning:
        cfg_nodec.clip_conditioning = True
    if clip_caps:
        cfg_nodec.clip_caps = True
        if clip_cap_count is None:
            clip_cap_count = 2
        cfg_nodec.clip_cap_count = clip_cap_count
    mdl_nodec = ImageModel(**cfg_nodec.__dict__)
    mdl_dec = mdl_nodec.clone(decode=True, attn_method=AttnMethod.STANDARD)

    img_toks = jax.random.randint(jax.random.PRNGKey(420), (image_tokens,), 0, 8192)
    if clip_conditioning:
        if clip_caps:
            clip_embedding = jax.random.normal(
                jax.random.PRNGKey(1337), (clip_cap_count, 768)
            )
            clip_embedding = clip_embedding / jnp.linalg.norm(
                clip_embedding, axis=-1, keepdims=True
            )
            max_cos_distance = jnp.full(clip_cap_count, 0.5)
        else:
            clip_embedding = jax.random.normal(jax.random.PRNGKey(1337), (768,))
            clip_embedding = clip_embedding / jnp.linalg.norm(clip_embedding)
            max_cos_distance = jnp.array([])
    else:
        clip_embedding = max_cos_distance = jnp.array([])

    params = mdl_nodec.init(
        jax.random.PRNGKey(69),
        images=img_toks[None, :],
        clip_embeddings=clip_embedding[None, :],
        max_cos_distances=max_cos_distance[None, :],
    )
    # IMPORTANT: use regular __call__ here, not decode_step. The cache needs to be initialized to
    # the full seq_len size.
    params_dec = mdl_dec.init(
        jax.random.PRNGKey(69),
        images=img_toks[None, :],
        clip_embeddings=clip_embedding[None, :],
        max_cos_distances=max_cos_distance[None, :],
    )

    _assert_dicts_equal(params["params"], params_dec["params"], "params")

    logits_all = mdl_nodec.apply(
        params,
        images=img_toks[None, :],
        clip_embeddings=clip_embedding[None, :],
        max_cos_distances=max_cos_distance[None, :],
    )[0]

    return (
        mdl_nodec,
        mdl_dec,
        params,
        params_dec["cache"],
        img_toks,
        clip_embedding,
        max_cos_distance,
        logits_all,
    )


def _test_sample_tok_0(
    clip_conditioning: bool, clip_caps: bool, clip_cap_count: Optional[int] = None
) -> None:
    """Test that step-by-step decoding is equivalent to all at once for image token 0."""
    (
        mdl_nodec,
        mdl_dec,
        params,
        cache,
        toks,
        clip_embedding,
        max_cos_distance,
        logits_all,
    ) = _setup_test_sample(clip_conditioning, clip_caps, clip_cap_count)

    params = flax.core.copy(params, {"cache": cache})
    logits_0, cache = mdl_dec.apply(
        params,
        mutable=["cache"],
        method=mdl_dec.decode_init,
        clip_embedding=clip_embedding,
        max_cos_distance=max_cos_distance,
    )
    assert logits_0.shape == (8192,)

    np.testing.assert_allclose(logits_all[0], logits_0, rtol=0, atol=1e-5)


def test_sample_tok_0_no_clip() -> None:
    _test_sample_tok_0(False, False)


def test_sample_tok_0_clip() -> None:
    _test_sample_tok_0(True, False)


def test_sample_tok_0_clip_caps_1() -> None:
    _test_sample_tok_0(True, True, 1)


def test_sample_tok_0_clip_caps_2() -> None:
    _test_sample_tok_0(True, True, 2)


def _test_sample_tok_1(clip_conditioning: bool, clip_caps: bool) -> None:
    """Test that step-by-step decoding is equivalent to all at once for token 1."""
    (
        mdl_nodec,
        mdl_dec,
        params,
        cache,
        toks,
        clip_embedding,
        max_cos_distance,
        logits_all,
    ) = _setup_test_sample(clip_conditioning, clip_caps)

    params = flax.core.copy(params, {"cache": cache})

    _logits_0, cache = mdl_dec.apply(
        params,
        mutable=["cache"],
        method=mdl_dec.decode_init,
        clip_embedding=clip_embedding,
        max_cos_distance=max_cos_distance,
    )
    params = flax.core.copy(params, cache)
    logits_1, _cache = mdl_dec.apply(
        params,
        mutable=["cache"],
        method=mdl_dec.decode_step,
        tok=toks[0],
        idx=jnp.array(0),
    )

    np.testing.assert_allclose(logits_all[1], logits_1, rtol=0, atol=1e-5)


def test_sample_tok_1_no_clip() -> None:
    _test_sample_tok_1(False, False)


def test_sample_tok_1_clip() -> None:
    _test_sample_tok_1(True, False)


def test_sample_tok_1_clip_caps() -> None:
    _test_sample_tok_1(True, True)


def _test_sample_tok_all(
    clip_conditioning: bool, clip_caps: bool, image_tokens: int = 256
) -> None:
    """Test that step-by-step decoding is equivalent to all at once for all tokens."""
    (
        mdl_nodec,
        mdl_dec,
        params,
        cache,
        toks,
        clip_embedding,
        max_cos_distance,
        logits_all,
    ) = _setup_test_sample(clip_conditioning, clip_caps, None, image_tokens)

    decoded_logits = []
    params = flax.core.copy(params, {"cache": cache})

    # compute logits for image tok 0
    logits, new_cache = mdl_dec.apply(
        params,
        mutable=["cache"],
        method=mdl_dec.decode_init,
        clip_embedding=clip_embedding,
        max_cos_distance=max_cos_distance,
    )
    assert logits.shape == (8192,)
    decoded_logits.append(logits)
    params = flax.core.copy(params, new_cache)

    step_j = jax.jit(
        lambda params, i: mdl_dec.apply(
            params,
            mutable=["cache"],
            method=mdl_dec.decode_step,
            tok=toks[i],
            idx=jnp.array(i),
        )
    )

    # compute logits for image toks 1-255 (inputting toks 0-254)
    for i in range(image_tokens - 1):
        logits, new_cache = step_j(params, i)
        assert logits.shape == (8192,)
        decoded_logits.append(logits)
        params = flax.core.copy(params, new_cache)

    decoded_logits = jnp.stack(decoded_logits, axis=0)
    assert decoded_logits.shape == (image_tokens, 8192)
    np.testing.assert_allclose(logits_all, decoded_logits, rtol=0, atol=1e-6)


def test_sample_tok_all_no_clip() -> None:
    _test_sample_tok_all(False, False)


def test_sample_tok_all_clip() -> None:
    _test_sample_tok_all(True, False)


def test_sample_tok_all_clip_caps() -> None:
    _test_sample_tok_all(True, True)


def test_sample_tok_all_clip_caps_1024() -> None:
    # There was a boundary issue with flash attention that broke with sequence lengths that > 1024
    # & not multiples of 1024.
    _test_sample_tok_all(True, True, 1024)


def test_clip_does_anything() -> None:
    """Test that changing the CLIP embedding changes the logits."""
    (
        mdl_nodec,
        mdl_dec,
        params,
        cache,
        toks,
        clip_embedding,
        max_cos_distance,
        logits_all,
    ) = _setup_test_sample(True, False)

    clip_embedding = jnp.zeros_like(clip_embedding)
    logits_all_zero = mdl_nodec.apply(
        params,
        images=toks[None, :],
        clip_embeddings=clip_embedding[None, :],
        max_cos_distances=jnp.array([[]]),
    )

    assert not jnp.allclose(logits_all, logits_all_zero, rtol=0, atol=1e-3)


def test_clip_caps_do_anything() -> None:
    """Test that changing the CLIP cap size changes the logits."""
    (
        mdl_nodec,
        mdl_dec,
        params,
        cache,
        toks,
        clip_embedding,
        max_cos_distance,
        logits_all,
    ) = _setup_test_sample(True, True)

    logits_full_range = mdl_nodec.apply(
        params,
        images=toks[None, :],
        clip_embeddings=clip_embedding[None, :],
        max_cos_distances=jnp.array([[2.0, 0.85]]),
    )

    assert not jnp.allclose(logits_all, logits_full_range, rtol=0, atol=1e-3)


@partial(jax.jit, static_argnums=(0,), inline=True)
def sample(
    mdl: ImageModel,
    params: dict[str, Any],
    clip_embedding: jax.Array,
    max_cos_distance: jax.Array,
    rng: jax.Array,
    top_p: float = 0.95,
) -> jax.Array:
    """Sample a single image from the model. Returns an array of codes to be passed to the
    LDM decoder."""
    if mdl.clip_conditioning and mdl.clip_caps:
        assert clip_embedding.shape == (mdl.clip_cap_count, 768)
        assert max_cos_distance.shape == (mdl.clip_cap_count,)
    elif mdl.clip_conditioning and not mdl.clip_caps:
        assert clip_embedding.shape == (768,)
        assert max_cos_distance.shape == (0,)
    else:
        assert clip_embedding.shape == max_cos_distance.shape == (0,)

    # Flash attention doesn't work with Flax's fast decoding. Something to do with how masks are
    # handled. Would be nice to fix it, but for now we just use the slower attention when sampling.
    mdl_decode = mdl.clone(decode=True, attn_method=AttnMethod.STANDARD, dropout=0.0)
    params_fake = mdl_decode.init(
        jax.random.PRNGKey(0),
        images=jnp.zeros((1, mdl.image_tokens), dtype=jnp.int32),
        clip_embeddings=clip_embedding[None, :],
        max_cos_distances=max_cos_distance[None, :],
    )
    params = flax.core.copy(params, {"cache": params_fake["cache"]})
    del params_fake

    # This needs to be outside the linen module because the fori_loop combinator doesn't work
    # inside them.
    def loop_iter(
        i: int, acc: Tuple[jax.Array, jax.Array]
    ) -> Tuple[jax.Array, jax.Array, dict[str, Any]]:
        image_toks, rng, params = acc
        logits, new_cache = mdl_decode.apply(
            params,
            mutable=["cache"],
            method=mdl_decode.decode_step,
            tok=image_toks[i],
            idx=i,
        )
        assert logits.shape == (8192,)
        params = flax.core.copy(params, new_cache)
        filtered_logits = _filter_top_p(logits, top_p)
        rng_sample, rng_loop = jax.random.split(rng, 2)
        tok = jax.random.categorical(rng_sample, filtered_logits)
        image_toks = image_toks.at[i + 1].set(tok)
        return (image_toks, rng_loop, params)

    rng0, rng_loop = jax.random.split(rng, 2)
    logits_0, cache = mdl_decode.apply(
        params,
        mutable=["cache"],
        method=mdl_decode.decode_init,
        clip_embedding=clip_embedding,
        max_cos_distance=max_cos_distance,
    )
    assert logits_0.shape == (8192,)
    filtered_logits_0 = _filter_top_p(logits_0, top_p)
    tok_0 = jax.random.categorical(rng0, filtered_logits_0)

    params = flax.core.copy(params, cache)

    image_toks = jnp.zeros((mdl.image_tokens,), dtype=jnp.int32).at[0].set(tok_0)
    image_toks, _, _ = jax.lax.fori_loop(  # type: ignore[no-untyped-call]
        0,
        mdl.image_tokens - 1,
        loop_iter,
        (image_toks, rng_loop, params),
    )
    return image_toks  # type: ignore[no-any-return]


def _filter_top_p(logits: jax.Array, top_p: float) -> jax.Array:
    """Filter an array of logits to include the smallest subset of possibilities that has
    proability mass at least p i.e. top p/nucleus sampling. Returns the filtered array.
    """
    probs = jax.nn.softmax(logits)
    sorted_probs, sorted_indices = (
        jnp.sort(probs)[::-1],
        jnp.argsort(probs)[::-1],
    )
    cumulative_probs = jnp.cumsum(sorted_probs)
    # collect the minimal set of possibilites with probability <= top_p
    mask = cumulative_probs <= top_p
    # Find the index of the first possibility that has cumulative probability >= top_p
    # this might be the last element we found above or might be the one after it.
    # we could do only the argmax and set the mask with a range but that's not JIT-able so we do
    # this.
    last_idx = jnp.argmax(cumulative_probs >= top_p)
    mask = mask.at[last_idx].set(True)

    # permute the mask back to the original order
    mask = mask[sorted_indices.argsort()]

    filtered_logits = jnp.where(mask, logits, -np.inf)
    return filtered_logits


def test_filter_top_p_10() -> None:
    """Test that filter_top_p is the identity function when top_p = 1.0."""
    logits = jnp.arange(10)
    filtered_logits = _filter_top_p(logits, 1.0)
    assert jnp.allclose(
        filtered_logits, logits
    ), "filter_top_p doesn't match the identity function when top_p = 1.0"


def test_filter_top_p_05() -> None:
    """Test that filter_top_p removes low-probability elements when top_p = 0.5."""
    probabilities = jnp.array([0.35, 0.35, 0.1, 0.1, 0.1])
    assert jnp.isclose(jnp.sum(probabilities), 1.0)
    logits = jnp.log(probabilities)
    filtered_logits = _filter_top_p(logits, 0.5)
    np.testing.assert_allclose(
        np.array(jax.nn.softmax(filtered_logits)), np.array([0.5, 0.5, 0, 0, 0])
    )


def test_filter_top_p_out_of_order() -> None:
    """Test that filter_top_p removes low-probability elements when inputs do not start sorted."""
    probabilities = np.repeat(1000.0, 7)
    big_indices = np.array([3, 5])
    medium_indices = np.array([2, 4])
    small_indices = np.array([0, 1, 6])
    probabilities[big_indices] = 0.25
    probabilities[medium_indices] = 0.2
    probabilities[small_indices] = 0.1 / 3.0
    np.testing.assert_allclose(np.sum(probabilities), 1.0)

    logits = jnp.log(probabilities)
    filtered_logits = _filter_top_p(logits, 0.75)
    filtered_probabilities = np.array(jax.nn.softmax(filtered_logits))

    np.testing.assert_allclose(filtered_probabilities[small_indices], 0.0)
    np.testing.assert_allclose(filtered_probabilities[medium_indices], 0.2 / 0.9)
    np.testing.assert_allclose(filtered_probabilities[big_indices], 0.25 / 0.9)


class TransformerLayer(nn.Module):
    """A single transformer layer."""

    d_model: int
    num_heads: int
    ff_dim: int
    dropout: Optional[float]
    use_biases: bool
    activations_dtype: jnp.dtype
    activation_function: Callable[[jax.Array], jax.Array]
    pre_norm: bool
    kernel_init: Callable[..., jnp.ndarray]
    decode: bool
    attn_method: AttnMethod

    def setup(self) -> None:
        if self.attn_method == AttnMethod.FLASH_JAX:
            # Use fast flash attention implementation
            def attn_function(
                q,
                k,
                v,
                bias=None,
                mask=None,
                broadcast_dropout=True,
                dropout_rng=None,
                dropout_rate=0.0,
                deterministic=False,
                dtype=None,
                precision=None,
            ):
                assert (
                    len(q.shape) == len(k.shape) == len(v.shape) == 4
                ), f"q k v shapes: {q.shape} {k.shape} {v.shape}, expected: (batch, seq_len, heads, head_dim)"
                assert (
                    q.shape[0] == k.shape[0] == v.shape[0]
                ), "batch dimensions must match"
                batch_size = q.shape[0]
                assert q.shape[1] == k.shape[1] == v.shape[1], "seq_len must match"
                seq_len = q.shape[1]
                assert q.shape[2] == k.shape[2] == v.shape[2], "num_heads must match"
                num_heads = q.shape[2]
                assert q.shape[3] == k.shape[3], "q & k head_dim must match"
                qk_head_dim, v_head_dim = q.shape[3], v.shape[3]

                rearrange_qkv = lambda x: rearrange(
                    x, "batch seq_len heads head_dim -> batch heads seq_len head_dim"
                )
                q, k, v = map(rearrange_qkv, (q, k, v))

                assert bias == None, "attention bias not implemented"
                assert (
                    mask == None
                ), "attention mask is redundant with causal_flash_attention"
                assert dropout_rate == 0.0, "attention dropout not implemented"

                try:
                    res = flash_attention_jax.causal_flash_attention(q, k, v)
                except TypeError as e:
                    if "cannot reshape array of shape" in str(e):
                        raise ValueError(
                            (
                                "Got an exception from causal_flash_attention: {}. You may have "
                                "run into its bug with sequence lengths that are not a multiple of "
                                "the chunk size."
                            ).format(e)
                        )
                if dtype != None:
                    assert res.dtype == dtype
                assert res.shape == (batch_size, num_heads, seq_len, v_head_dim)
                res = rearrange(
                    res, "batch heads seq_len head_dim -> batch seq_len heads head_dim"
                )
                assert res.shape == (batch_size, seq_len, num_heads, v_head_dim)
                return res

        elif self.attn_method == AttnMethod.FLASH_CPP:
            def attn_function(
                q,
                k,
                v,
                bias=None,
                mask=None,
                broadcast_dropout=True,
                dropout_rng=None,
                dropout_rate=0.0,
                deterministic=False,
                dtype=None,
                precision=None,
            ):
                assert bias == None, "attention bias not implemented"
                assert mask == None, "attention mask is redundant with causal_flash_attention"
                assert dropout_rate == 0.0, "attention dropout not implemented"
                assert dtype in [jnp.bfloat16, jnp.float16], "CPP flash attention only supports bfloat16 & float16"

                res = flash_attention_cpp.flash_mha(q, k, v, is_causal=True)
                assert res.shape == v.shape
                return res

        elif self.attn_method == AttnMethod.STANDARD:
            attn_function = nn.attention.dot_product_attention
        else:
            raise ValueError(f"Invalid attention method: {self.attn_method}")

        self.mha = nn.SelfAttention(
            num_heads=self.num_heads,
            qkv_features=self.d_model,
            # dropout in the attention matrix was introduced in
            # https://arxiv.org/abs/1907.11065, it's *not* the normal thing
            # from Attention is All You Need.
            dropout_rate=0,
            deterministic=False,
            use_bias=self.use_biases,
            dtype=self.activations_dtype,
            kernel_init=self.kernel_init,
            decode=self.decode,
            attention_fn=attn_function,
        )
        self.layer_norm_1 = nn.LayerNorm(dtype=self.activations_dtype)
        self.linear_1 = nn.Dense(
            features=self.ff_dim,
            use_bias=self.use_biases,
            kernel_init=self.kernel_init,
            dtype=self.activations_dtype,
        )
        self.linear_2 = nn.Dense(
            features=self.d_model,
            use_bias=self.use_biases,
            kernel_init=self.kernel_init,
            dtype=self.activations_dtype,
        )
        self.layer_norm_2 = nn.LayerNorm(dtype=self.activations_dtype)
        if self.dropout is not None:
            self.dropout_layer = nn.Dropout(self.dropout, deterministic=False)
        else:
            self.dropout_layer = nn.Dropout(rate=0, deterministic=True)

    def __call__(self, embeds: jax.Array, _) -> jax.Array:
        assert_msg = (
            f"embeds.shape: {embeds.shape}, expected: (batch, seq_len, {self.d_model})"
        )
        assert len(embeds.shape) == 3, assert_msg
        assert embeds.shape[2] == self.d_model, assert_msg
        batch_size = embeds.shape[0]
        seq_len = embeds.shape[1]

        if self.attn_method == AttnMethod.FLASH_JAX or self.attn_method == AttnMethod.FLASH_CPP:
            mask = None
        else:
            mask = jnp.tril(
                jnp.ones((batch_size, self.num_heads, embeds.shape[1], embeds.shape[1]))
            )

        if self.pre_norm:
            embeds = self.layer_norm_1(embeds)
        attn_output = self.mha(embeds, mask=mask)
        if not self.pre_norm:
            attn_output = self.layer_norm_1(attn_output)
        embeds = embeds + self.dropout_layer(attn_output)

        if self.pre_norm:
            embeds = self.layer_norm_2(embeds)
        ff_output = self.linear_2(self.activation_function(self.linear_1(embeds)))
        if not self.pre_norm:
            ff_output = self.layer_norm_2(ff_output)
        embeds = embeds + self.dropout_layer(ff_output)

        assert embeds.shape == (batch_size, seq_len, self.d_model)
        return embeds, None

@pytest.mark.parametrize("flash_method", [
    pytest.param(AttnMethod.FLASH_JAX),
    pytest.param(AttnMethod.FLASH_CPP, marks=pytest.mark.requires_ampere_or_newer)])
def test_flash_attention_equals_standard(flash_method: AttnMethod) -> None:
    """Test that flash attention gives the same results as Flax's standard attention."""
    activations_dtype = jnp.float32 if flash_method == AttnMethod.FLASH_JAX else jnp.bfloat16
    mdl_std = TransformerLayer(
        d_model=768,
        num_heads=12,
        ff_dim=3072,
        dropout=None,
        use_biases=False,
        activations_dtype=activations_dtype,
        activation_function=jax.nn.relu,
        pre_norm=False,
        kernel_init=jax.nn.initializers.xavier_uniform(),
        decode=False,
        attn_method=AttnMethod.STANDARD,
    )

    input_shape = (4, 64, 768)
    input_vals = jax.random.normal(jax.random.PRNGKey(0), input_shape)

    params = mdl_std.init(
        jax.random.PRNGKey(1), jnp.ones(input_shape, dtype=jnp.float32), None
    )

    out_std, _ = mdl_std.apply(params, input_vals, None)

    mdl_flash = mdl_std.clone(attn_method=flash_method)
    out_flash, _ = mdl_flash.apply(params, input_vals, None)

    np.testing.assert_allclose(out_std, out_flash, atol=1e-5, rtol=0)


def loss_batch_tokens(
    model: ImageModel,
    params: dict[str, Any],
    dropout_rng: jax.Array,
    batch_imgs: jax.Array,
    batch_clips: jax.Array,
    batch_max_cos_distances: jax.Array,
) -> jax.Array:
    """Compute the cross-entropy loss for each token in a batch of examples."""
    batch_size = batch_imgs.shape[0]
    assert batch_imgs.shape == (
        batch_size,
        model.image_tokens,
    ), f"batch_img.shape: {batch_imgs.shape}, expected: {(batch_size, model.image_tokens)}"
    if model.clip_conditioning and not model.clip_caps:
        assert batch_clips.shape == (batch_size, 768)
        assert batch_max_cos_distances.shape == (batch_size, 0)
    elif model.clip_conditioning and model.clip_caps:
        assert batch_clips.shape == (batch_size, model.clip_cap_count, 768)
        assert batch_max_cos_distances.shape == (batch_size, model.clip_cap_count)
    else:
        assert batch_clips.shape == batch_max_cos_distances.shape == (batch_size, 0)
    logits: jax.Array = model.apply(
        params,
        rngs={"dropout": dropout_rng},
        images=batch_imgs,
        clip_embeddings=batch_clips,
        max_cos_distances=batch_max_cos_distances,
    )
    per_token_loss = optax.softmax_cross_entropy(
        logits, jax.nn.one_hot(batch_imgs, 8192)
    )
    assert per_token_loss.shape == (
        batch_size,
        model.image_tokens,
    ), f"per_token_loss.shape: {per_token_loss.shape}"
    return per_token_loss


def loss_batch(
    model: ImageModel,
    params: dict[str, Any],
    loss_decay_constant: float,
    dropout_rng: jax.Array,
    batch_imgs: jax.Array,
    batch_clips: jax.Array,
    batch_max_cos_distances: jax.Array,
) -> jax.Array:
    """Compute the weighted cross-entropy loss for a batch of examples. The loss is scaled by
    loss_decay_constant such that the last token is weighted loss_decay_constant as much as the
    first one and the rest are exponentially decayed in between. loss_decay_constant should be in
    (0, 1] with 1 meaning no decay."""
    per_token_loss = loss_batch_tokens(
        model, params, dropout_rng, batch_imgs, batch_clips, batch_max_cos_distances
    )
    token_weights = loss_decay_constant ** jnp.linspace(0, 1, num=model.image_tokens)
    # Normalize so the total weight is always equal to what it'd be with no decay
    token_weights = token_weights / token_weights.sum() * model.image_tokens
    assert token_weights.shape == (
        model.image_tokens,
    ), f"token_weights.shape: {token_weights.shape}, expected: {(model.image_tokens,)}"
    weighted_loss = per_token_loss * token_weights[None, :]
    assert weighted_loss.shape == (
        per_token_loss.shape[0],
        model.image_tokens,
    ), f"weighted_loss.shape: {weighted_loss.shape}, expected: {(per_token_loss.shape[0], model.image_tokens)}"
    out = jnp.mean(weighted_loss)
    assert out.shape == (), f"out.shape: {out.shape}, expected scalar"
    return out


# Parameters taken from GPT-1, except seq_len is 256 instead of 1024
gpt_1_config = ModelConfig(
    d_model=768,
    num_heads=12,
    ff_dim=3072,
    dropout=0.1,
    n_layers=12,
    image_tokens=256,
    use_biases=True,
    activation_function=jax.nn.relu,
    clip_conditioning=False,
)


def test_cap_train() -> None:
    """Test the model can memorize some image/clip pairs."""
    mdl_cfg = copy(gpt_1_config)
    mdl_cfg.clip_conditioning = True
    mdl_cfg.clip_caps = True
    mdl_cfg.clip_cap_count = 9
    mdl_cfg.dropout = None
    # This may or may not actually need a model this big, I'm tired of fucking with it.
    mdl_cfg.d_model = 1024
    mdl_cfg.num_heads = 16
    mdl_cfg.n_layers = 24

    n_imgs = 8

    mdl = ImageModel(**mdl_cfg.__dict__)

    (
        img_rng,
        clip_rng,
        max_cos_distance_rng,
        params_rng,
        train_rng,
        test_rng,
    ) = jax.random.split(jax.random.PRNGKey(0), 6)

    imgs = jax.random.randint(img_rng, (n_imgs, mdl.image_tokens), 0, 8192)
    clips = jax.random.normal(clip_rng, (n_imgs, mdl.clip_cap_count, 768))
    clips = clips / jnp.linalg.norm(clips, axis=-1, keepdims=True)
    max_cos_distances = jax.random.uniform(
        max_cos_distance_rng, shape=(n_imgs, mdl.clip_cap_count), minval=0.0, maxval=2.0
    )

    params = mdl.init(
        {"params": params_rng, "dropout": jax.random.PRNGKey(0)},
        images=jnp.zeros((1, mdl.image_tokens), dtype=jnp.int32),
        clip_embeddings=jnp.zeros((1, mdl.clip_cap_count, 768), dtype=jnp.float32),
        max_cos_distances=jnp.zeros((1, mdl.clip_cap_count), dtype=jnp.float32),
    )

    loss_grad_fn = jax.value_and_grad(loss_batch, argnums=1)

    steps = 500
    adam = optax.adam(learning_rate=triangle_schedule(3e-5, steps))
    opt = optax.chain(optax.clip_by_global_norm(0.25), adam)
    opt_state = opt.init(params)

    def opt_step(params, opt_state, rng):
        dropout_rng, rng2 = jax.random.split(rng, 2)
        loss, grads = loss_grad_fn(
            mdl,
            params,
            1.0,
            dropout_rng,
            batch_imgs=imgs,
            batch_clips=clips,
            batch_max_cos_distances=max_cos_distances,
        )
        updates, opt_state = opt.update(grads, opt_state)
        new_params = optax.apply_updates(params, updates)
        norm = optax.global_norm(grads)
        return new_params, opt_state, rng2, loss, norm

    opt_step = jax.jit(opt_step, donate_argnums=(0, 1, 2))

    for i in trange(steps):
        params, opt_state, train_rng, loss, norm = opt_step(
            params, opt_state, train_rng
        )
        tqdm.write(f"iter {i:04d} loss: {loss:0.4f} grad norm: {norm:7.2f}")

    sampled_imgs = []
    for i in trange(n_imgs):
        sample_rng, test_rng = jax.random.split(test_rng, 2)

        # This only tests the ability to memorize training examples, and not even ability to
        # generalize to different sets of caps for the same image.

        toks = sample(
            mdl,
            params,
            clips[i],
            max_cos_distances[i],
            sample_rng,
            top_p=0.25,
        )
        sampled_imgs.append(toks)

    sampled_imgs = jnp.stack(sampled_imgs, axis=0)

    # Test that sampled images are the training images
    correct_imgs = [0] * n_imgs
    for img in sampled_imgs:
        for i, train_img in enumerate(imgs):
            if jnp.all(img == train_img):
                correct_imgs[i] += 1
                break
    correct_img_cnt = sum(correct_imgs)
    print(f"correct_imgs: {correct_imgs}, count {correct_img_cnt}")
    assert correct_img_cnt == n_imgs

    # Test that sampled images match the CLIP conditioning
    correct_conds = [False] * n_imgs
    for i in range(n_imgs):
        correct_conds[i] = jnp.array_equal(imgs[i], sampled_imgs[i])
    correct_conds = np.array(correct_conds)
    print(f"correct_conds: {correct_conds}")
    assert all(correct_conds)


def train_loop_simple(
    data: jax.Array,
    mdl: ImageModel,
    iters: int,
    learning_rate: float = 3e-5,
    loss_decay_constant: float = 1.0,
) -> Tuple[jax.Array, dict[str, Any]]:
    """Train the model repeatedly on a single batch for testing."""
    assert mdl.clip_conditioning is False
    assert len(data.shape) == 2
    batch_size = data.shape[0]
    assert data.shape == (batch_size, mdl.image_tokens)
    params = mdl.init(
        rngs={"dropout": jax.random.PRNGKey(0), "params": jax.random.PRNGKey(1)},
        images=jnp.zeros((1, gpt_1_config.image_tokens), dtype=jnp.int32),
        clip_embeddings=jnp.zeros((1, 0), dtype=jnp.float32),
        max_cos_distances=jnp.zeros((1, 0), dtype=jnp.float32),
    )
    opt = optax.adam(learning_rate=learning_rate)
    opt_state = opt.init(params)
    loss_grad_fn = jax.value_and_grad(loss_batch, argnums=1)

    def opt_step(
        params: dict[str, Any],
        opt_state: Any,
        rng: jax.Array,
        batch_imgs: jax.Array,
    ) -> Tuple[dict[str, Any], Any, jax.Array, jax.Array]:
        dropout_rng, rng2 = jax.random.split(rng, 2)
        loss, grads = loss_grad_fn(
            mdl,
            params,
            loss_decay_constant,
            dropout_rng,
            batch_imgs=batch_imgs,
            batch_clips=jnp.zeros((batch_imgs.shape[0], 0), dtype=jnp.float32),
            batch_max_cos_distances=jnp.zeros(
                (batch_imgs.shape[0], 0), dtype=jnp.float32
            ),
        )
        updates, opt_state = opt.update(grads, opt_state)
        new_params: dict[str, Any] = optax.apply_updates(params, updates)
        return new_params, opt_state, rng2, loss

    opt_step = jax.jit(opt_step, donate_argnums=(0, 1, 2))

    train_rng = jax.random.PRNGKey(0)
    for i in range(iters):
        params, opt_state, train_rng, loss = opt_step(  # type:ignore[misc]
            params, opt_state, train_rng, data
        )
        print(f"iter {i} loss: {loss}")
    return loss, params  # type:ignore[return-value]


@pytest.mark.parametrize("pre_norm", [True, False])
def test_learn_zeros(pre_norm: bool) -> None:
    """Test whether the model can learn to predict all zeros."""
    mdl_cfg = copy(gpt_1_config)
    mdl_cfg.pre_norm = pre_norm
    mdl = ImageModel(**mdl_cfg.__dict__)
    data = jnp.zeros((16, gpt_1_config.image_tokens), dtype=jnp.int32)
    loss, params = train_loop_simple(data, mdl, 20, learning_rate=3e-3)
    assert loss < 1e-10

    sampled_arr = sample(
        mdl,
        params,
        jnp.zeros((0,), dtype=jnp.float32),
        jnp.array([], dtype=jnp.float32),
        jax.random.PRNGKey(0),
    )
    assert jnp.all(sampled_arr == 0)


@pytest.mark.parametrize("pre_norm", [True, False])
def test_learn_ranges(pre_norm: bool) -> None:
    """Test whether the model can memorize ranges of integers."""
    mdl_cfg = copy(gpt_1_config)
    mdl_cfg.pre_norm = pre_norm
    mdl = ImageModel(**mdl_cfg.__dict__)
    # The numbers are sequential but there's nothing about the way the model works that tells it
    # the ordering of the tokens in the codebook, so this is a pure memorization test.
    data = jnp.arange(16 * gpt_1_config.image_tokens).reshape(
        (16, gpt_1_config.image_tokens)
    )
    loss, params = train_loop_simple(
        data, mdl, 200 if pre_norm else 400, learning_rate=3e-4 if pre_norm else 3e-5
    )
    assert loss < 0.6
    empty_arr = jnp.zeros((0,), dtype=jnp.float32)
    sample_jv = jax.jit(
        lambda params, rng: jax.vmap(
            lambda rng: sample(mdl, params, empty_arr, empty_arr, rng, top_p=0.98)
        )(rng)
    )
    print("Generating samples...")
    total_samples = 256
    samples_per_batch = 64
    assert total_samples % samples_per_batch == 0
    sample_batches = []
    rng = jax.random.PRNGKey(0)
    for _ in trange(total_samples // samples_per_batch):
        rng, rng2 = jax.random.split(rng)
        sample_batches.append(
            sample_jv(params, jax.random.split(rng2, samples_per_batch))
        )
    sampled_arr: jax.Array = jnp.concatenate(sample_batches, axis=0)
    print(f"Generated samples, shape: {sampled_arr.shape}")
    counts = dict([(i, 0) for i in range(16)])
    for i, s in enumerate(sampled_arr):
        assert s[0] % 256 == 0, f"sample {i} starts with {s[0]}"
        np.testing.assert_equal(np.array(s), np.array(s[0] + np.arange(256)))
        counts[int(s[0] // 256)] += 1
    print(f"counts: {counts}")
    assert all([c > 0 for c in counts.values()])


def test_loss_weighting() -> None:
    """Train a model with no weighting to memorize a random data set, and then one with strong
    weighting and check that the weighted model learns the early tokens faster."""
    mdl = ImageModel(**gpt_1_config.__dict__)
    data = jax.random.randint(
        jax.random.PRNGKey(0), (16, gpt_1_config.image_tokens), 0, 8192
    )
    _loss_unweighted, params_unweighted = train_loop_simple(
        data, mdl, 250, loss_decay_constant=1.0
    )
    _loss_weighted, params_weighted = train_loop_simple(
        data, mdl, 250, loss_decay_constant=0.1
    )

    def loss_first_half(params):
        # Compute the loss on the first half of the tokens
        losses = loss_batch_tokens(
            mdl,
            params,
            jax.random.PRNGKey(0),
            data,
            jnp.zeros((16, 0), dtype=jnp.float32),
            jnp.zeros((16, 0), dtype=jnp.float32),
        )
        assert losses.shape == (16, gpt_1_config.image_tokens)
        losses_first_half = losses[:, : gpt_1_config.image_tokens // 2]
        return jnp.mean(losses_first_half)

    loss_unweighted_first_half = loss_first_half(params_unweighted)
    loss_weighted_first_half = loss_first_half(params_weighted)
    print(
        f"first half losses: unweighted {loss_unweighted_first_half}, weighted {loss_weighted_first_half}"
    )
    assert loss_unweighted_first_half > loss_weighted_first_half
    assert loss_weighted_first_half < 0.1


def test_clip_caps_overfit():
    """Using a collection of ~100k, cap-image pair training examples, train a model and check test
    loss, then sample and check the generated samples. The training data is 100k pairs generated
    from 100 images (drawn with replacement). This tests the model's ability to memorize a small
    set of image encodings and learn to sample based on their CLIP encodings being within caps.
    The trained model should be overfit on the images, but not overfit on the caps. We test against
    a holdout set of cap-image pairs from the same 100 images."""

    params_rng, sample_rng = jax.random.split(jax.random.PRNGKey(42), 2)
    # run get-test-data.sh to download this
    dset_all = load_pq_to_infinidata(
        Path(__file__).parent.parent / "test-images/examples-100.pq"
    ).shuffle(seed=420_69)
    dset_train = dset_all.new_view(slice(None, -16))
    dset_test = dset_all.new_view(slice(-16, None))

    mdl_cfg = copy(gpt_1_config)
    mdl_cfg.clip_conditioning = True
    mdl_cfg.clip_caps = True
    mdl_cfg.clip_cap_count = 1
    mdl_cfg.dropout = None
    mdl_cfg.d_model = 1024
    mdl_cfg.num_heads = 16
    mdl_cfg.n_layers = 24

    mdl = ImageModel(**mdl_cfg.__dict__)

    params = _train_clip_caps_overfit(dset_train, mdl, params_rng)
    _test_clip_caps_overfit_test_loss(dset_test, mdl, params)
    _test_clip_caps_overfit_samples(dset_test, mdl, params, sample_rng)


def _train_clip_caps_overfit(dset_train, mdl, rng):
    """Train a test model"""
    params_rng, dropout_rng = jax.random.split(rng, 2)

    params = mdl.init(
        {"params": params_rng},
        images=jnp.zeros((1, mdl.image_tokens), dtype=jnp.int32),
        clip_embeddings=jnp.zeros((1, mdl.clip_cap_count, 768), dtype=jnp.float32),
        max_cos_distances=jnp.zeros((1, mdl.clip_cap_count), dtype=jnp.float32),
    )

    loss_grad_fn = jax.value_and_grad(loss_batch, argnums=1)

    steps = 2_000
    batch_size = 32

    adam = optax.adam(learning_rate=triangle_schedule(1e-4, steps))
    opt = optax.chain(optax.clip_by_global_norm(0.25), adam)
    opt_state = opt.init(params)

    def opt_step(params, opt_state, rng, images, clips, max_cos_distances):
        dropout_rng, rng2 = jax.random.split(rng, 2)
        loss, grads = loss_grad_fn(
            mdl,
            params,
            1.0,
            dropout_rng,
            batch_imgs=images,
            batch_clips=clips,
            batch_max_cos_distances=max_cos_distances,
        )
        updates, opt_state = opt.update(grads, opt_state)
        new_params = optax.apply_updates(params, updates)
        norm = optax.global_norm(grads)
        return new_params, opt_state, rng2, loss, norm

    opt_step = jax.jit(opt_step, donate_argnums=(0, 1, 2))

    with tqdm(total=steps) as pbar:
        while pbar.n < steps:
            dset_shuf = dset_train.shuffle()
            for batch in dset_shuf.batch_iter(batch_size, drop_last_batch=True):
                images, cap_centers, max_cos_distances = (
                    batch["encoded_img"],
                    rearrange(batch["cap_center"], "b c -> b 1 c"),
                    rearrange(batch["cap_max_cos_distance"], "b -> b 1"),
                )
                assert (
                    images.shape[0]
                    == cap_centers.shape[0]
                    == max_cos_distances.shape[0]
                )
                params, opt_state, dropout_rng, loss, norm = opt_step(
                    params,
                    opt_state,
                    dropout_rng,
                    images,
                    cap_centers,
                    max_cos_distances,
                )
                loss, norm = np.asarray(loss), np.asarray(norm)
                pbar.update(1)
                pbar.set_postfix({"loss": loss, "grad norm": norm})
                if pbar.n % 100 == 0:
                    tqdm.write(
                        f"iter {pbar.n:04d} loss: {loss:0.4f} grad norm: {norm:05.2f}"
                    )
                if pbar.n >= steps:
                    break
    return params


def _test_clip_caps_overfit_test_loss(dset_test, mdl, params):
    """Compute test loss and assert it's low enough"""
    examples = dset_test[:]
    cap_centers = rearrange(examples["cap_center"], "b c -> b 1 c")
    max_cos_distances = rearrange(examples["cap_max_cos_distance"], "b -> b 1")
    imgs = examples["encoded_img"]

    computed_loss = loss_batch(
        mdl, params, 1.0, jax.random.PRNGKey(0), imgs, cap_centers, max_cos_distances
    )
    print(f"test loss: {computed_loss}")
    assert computed_loss < 0.05


def _test_clip_caps_overfit_samples(dset_test, mdl, params, rng):
    """Sample from the model and check it returns images with embeddings inside the test caps."""
    pt_rng, sample_rng = jax.random.split(rng, 2)

    examples = dset_test[:]
    n_samples = len(examples["encoded_img"])
    embeddings = examples["clip_embedding"]

    query_pts = jax.vmap(random_pt_with_cosine_similarity, in_axes=(0, 0, None))(
        jax.random.split(pt_rng, n_samples), embeddings, 0.8
    )
    query_max_cos_distances = jnp.full((n_samples,), 0.25)

    matches = np.zeros((n_samples,), dtype=np.bool_)
    for i in trange(n_samples):
        sample_rng, sub_rng = jax.random.split(sample_rng, 2)
        toks = sample(
            mdl,
            params,
            query_pts[i : i + 1],
            query_max_cos_distances[i : i + 1],
            sub_rng,
            top_p=0.05,
        )
        matches[i] = np.array_equal(toks, examples["encoded_img"][i])

    print(f"Found {matches.sum()} matches out of {n_samples} samples")
    assert matches.sum() >= round(n_samples * 0.9)
