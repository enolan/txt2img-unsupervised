import flax.core
import flax.linen as nn
import jax
import jax.numpy as jnp
import numpy as np
import optax  # type: ignore[import]
from copy import copy
from dataclasses import dataclass
from datetime import datetime
from einops import rearrange, repeat
from flash_attention_jax import causal_flash_attention
from flax import struct
from functools import partial
from infinidata import TableView
from pathlib import Path
from typing import Any, Callable, Optional, Tuple
from tqdm import tqdm, trange

from .cone_sampling import random_pt_with_cosine_similarity
from .config import ModelConfig
from .gen_training_caps import gen_training_examples_from_tree
from .load_pq_dir import load_pq_to_infinidata
from .spherical_space_partitioning import CapTree
from .triangle_schedule import triangle_schedule


class ImageModel(nn.Module):
    """A transformer model for images encoded to a discrete representation."""

    d_model: int
    num_heads: int
    ff_dim: int
    dropout: Optional[float]
    n_layers: int
    image_tokens: int
    clip_conditioning: bool
    clip_caps: bool
    clip_cap_count: Optional[int]
    use_biases: bool
    activations_dtype: jnp.dtype
    activation_function: Callable[[jax.Array], jax.Array]
    decode: bool = False
    flash_attention: bool = True

    def setup(self) -> None:
        default_stddev = 0.02 / jnp.sqrt(self.n_layers)
        default_kernel_init = nn.initializers.normal(stddev=default_stddev)
        self.in_embed = nn.Embed(
            num_embeddings=8192,
            features=self.d_model,
            embedding_init=default_kernel_init,
            dtype=self.activations_dtype,
        )
        # A note on how CLIP conditioning works:
        # There are three modes:
        # 1) No conditioning. We prepend a zero token to the input sequence.

        # 2) Conditioning on one CLIP embedding. We project the CLIP embedding into d_model
        # and prepend it to the input sequence. This works, and the model learns to produce images
        # that look like an image with that CLIP embedding.

        # 3) Conditioning on one or more spherical caps. CLIP embeddings are unit vectors, and the
        # image's embedding must fall inside all the caps. We project the cap's center and the
        # maximum cosine distance the embedding can have from the center into d_model, and then
        # sum them, producing one conditioning token per cap, which we then prepend to the input.
        # So the image's embedding must be in the intersection of all the caps. I'm trying this
        # for a few reasons: firstly, because an ideal model conditioning on a single exact
        # embedding would learn to produce images with that exact embedding, when in reality
        # image-text prompt cosine similarities are usually between like 0.15 and 0.4 and
        # image-very similiar image cosine similarities are around 0.8. Secondly, because the
        # intersections may allow you to do things with prompting that are impossible with a single
        # prompt. Other models do allow multiple prompts but AFAICT they're not doing
        # *intersections* they're just taking an average. Thirdly, this gives us something similar
        # to classifier free guidance - prompting with bigger caps is like lower cfg scale and
        # will hopefully lead to more realistic - though less prompt-aligned - images.
        # N.B. the more caps we condition on the more the model learns about the CLIP embeddings
        # of the examples at train time. One cap may be a very large area, but more allows it to
        # effectively "triangulate".

        # At training time we use precomputed cap-image pairs, generated by gen_training_caps.py

        # 1 & 2 work, we'll find out whether 3 does soon, hopefully.

        assert (
            self.clip_conditioning or not self.clip_caps
        ), "Can't use clip_caps without clip_conditioning"

        if self.clip_caps:
            assert self.clip_cap_count is not None, "clip_cap_count must be set"
            assert self.clip_cap_count > 0, "clip_cap_count must be positive"

        # The initializers for CLIP conditioning are chosen such that the projected clip emedding
        # has the same distribution as the token embeddings and the projected max cosine distances
        # have the same average magnitude as the token embeddings, assuming the distances are drawn
        # from U[0, 2]. The max distances need to be converted into min similarities for the math
        # to work.
        self.clip_proj = nn.Dense(
            features=self.d_model,
            use_bias=self.use_biases,
            dtype=self.activations_dtype,
            kernel_init=default_kernel_init,
        )
        self.max_cos_distance_proj = nn.Dense(
            features=self.d_model,
            use_bias=self.use_biases,
            dtype=self.activations_dtype,
            kernel_init=nn.initializers.normal(stddev=2 * default_stddev),
        )
        self.positional_encoding = nn.Embed(
            num_embeddings=self.seq_len(),
            features=self.d_model,
            embedding_init=default_kernel_init,
            dtype=self.activations_dtype,
        )
        # it'd potentially be better to use nn.remat_scan here, but it makes inference massively
        # slower for some reason. Even though checkpointing should only affect gradient computation.
        # Might have to do with the fact that remat_scan creates a scan-of-scans? Could cause bad
        # optimization in JAX or XLA.
        self.transformer_layers = nn.scan(
            nn.remat(TransformerLayer),
            variable_axes={"params": 0, "cache": 0},
            variable_broadcast=False,
            split_rngs={"params": True, "dropout": True},
            length=self.n_layers,
        )(
            d_model=self.d_model,
            num_heads=self.num_heads,
            ff_dim=self.ff_dim,
            dropout=self.dropout,
            use_biases=self.use_biases,
            activations_dtype=self.activations_dtype,
            activation_function=self.activation_function,
            kernel_init=default_kernel_init,
            decode=self.decode,
            flash_attention=self.flash_attention,
        )

        self.logits_decoder = nn.Dense(
            features=8192,
            kernel_init=default_kernel_init,
            use_bias=self.use_biases,
            dtype=self.activations_dtype,
        )

        tokens_res = int(self.image_tokens**0.5)
        assert tokens_res * tokens_res == self.image_tokens

    def seq_len(self) -> int:
        """How many tokens are in the sequence being modeled."""
        return self.image_tokens + self.prepended_tokens() - 1

    def prepended_tokens(self) -> int:
        """How many tokens are prepended to the image tokens."""
        # We always prepend at least one, becuase the first image token must be conditioned on
        # *something* even when that's a constant.
        if self.clip_conditioning:
            if self.clip_caps:
                return self.clip_cap_count
            else:
                return 1
        else:
            return 1

    def gen_conditioning_tokens(
        self,
        clip_embedding: jax.Array,
        max_cos_distance: jax.Array,
    ) -> jax.Array:
        """Generate the conditioning tokens that should be prepended to the image tokens. Returns a
        (self.prepended_tokens(), self.d_model) shaped array."""
        if not self.clip_conditioning:
            assert clip_embedding.shape == max_cos_distance.shape == (0,)
            res = jnp.zeros((1, self.d_model), dtype=self.activations_dtype)
        else:
            if not self.clip_caps:
                assert clip_embedding.shape == (768,)
                assert max_cos_distance.shape == (0,)
                res = self.clip_proj(clip_embedding)[None, :]
            else:
                assert clip_embedding.shape == (self.clip_cap_count, 768)
                assert max_cos_distance.shape == (self.clip_cap_count,)
                res = (
                    self.clip_proj(clip_embedding)
                    + self.max_cos_distance_proj(1 - max_cos_distance)
                ) / 2
        assert res.shape == (self.prepended_tokens(), self.d_model)
        return res

    def output_shape_tokens(self) -> int:
        """What (2-D) shape of tokens is output by the model."""
        res = int(self.image_tokens**0.5)
        return (res, res)

    def __call__(
        self, image: jax.Array, clip_embedding: jax.Array, max_cos_distance
    ) -> jax.Array:
        """Run the model, returning log probabilities of the image tokens. No probabilities are computed
        for any CLIP conditioning tokens."""
        assert image.shape == (
            self.image_tokens,
        ), f"Expected image shape {(self.image_tokens,)}, got {image.shape}"
        assert image.dtype == jnp.int32 or image.dtype == jnp.int64

        embeds = self.in_embed(image)

        cond_tokens = self.gen_conditioning_tokens(clip_embedding, max_cos_distance)
        toks = jnp.concatenate([cond_tokens, embeds[:-1]], axis=0)
        assert toks.shape == (self.seq_len(), self.d_model)

        h: jax.Array = toks + self.positional_encoding(jnp.arange(self.seq_len()))
        h, _ = self.transformer_layers(h, None)
        h = h[self.prepended_tokens() - 1 :]
        h = self.logits_decoder(h)
        assert h.shape == (self.image_tokens, 8192)

        return h

    def decode_init(
        self,
        clip_embedding: jax.Array,
        max_cos_distance: jax.Array,
    ):
        """Initialize the cache for decoding by computing and feeding the conditioning tokens. Returns
        the logits for the first image token. The cache should be ready for use with decode_step
        when this is done."""
        assert self.decode
        assert not self.flash_attention, "Flash attention doesn't work with decoding."

        cond_tokens = self.gen_conditioning_tokens(clip_embedding, max_cos_distance)

        h = cond_tokens + self.positional_encoding(jnp.arange(self.prepended_tokens()))
        assert h.shape == (self.prepended_tokens(), self.d_model)

        # TODO vectorize, don't loop
        for tok in h:
            h, _ = self.transformer_layers(tok[None, :], None)
            last_tok = h[0]
        assert last_tok.shape == (self.d_model,)

        logits_out = self.logits_decoder(last_tok)
        assert logits_out.shape == (8192,)
        return logits_out

    def decode_step(self, tok: jax.Array, idx: jax.Array) -> jax.Array:
        """Do a step of iterative decoding from the model. Returns the logits for the next token.
        See below tests for usage examples.
        """
        assert (
            self.decode
        ), "Can't call decode_step on a model that wasn't set up for decoding."
        assert not self.flash_attention, "Flash attention doesn't work with decoding."
        assert tok.shape == ()
        assert tok.dtype == jnp.int32 or tok.dtype == jnp.int64
        assert idx.shape == ()

        embed = self.in_embed(tok)
        assert embed.shape == (self.d_model,)

        h = embed + self.positional_encoding(idx + self.prepended_tokens())
        assert h.shape == (self.d_model,)
        h = h[None, :]

        h, _ = self.transformer_layers(h, None)
        return self.logits_decoder(h[0])  # type: ignore[no-any-return]


def _assert_dicts_equal(d1, d2, name) -> None:
    assert isinstance(d1, dict)
    assert isinstance(d2, dict)
    assert d1.keys() == d2.keys()
    for k in d1.keys():
        if isinstance(d1[k], dict):
            _assert_dicts_equal(d1[k], d2[k], f"{name}.{k}")
        elif isinstance(d1[k], jax.Array):
            np.testing.assert_allclose(
                np.array(d1[k]), np.array(d2[k]), atol=1e-8, rtol=0
            )
        else:
            assert False, f"unknown type {type(d1[k])} for {name}.{k}"


def _setup_test_sample(
    clip_conditioning: bool = False,
    clip_caps: bool = False,
    clip_cap_count: Optional[int] = None,
    image_tokens: int = 256,
) -> Tuple[ImageModel, ImageModel, dict, jax.Array, jax.Array]:
    """Shared setup code for iterative sampling tests."""
    cfg_nodec = copy(gpt_1_config)
    cfg_nodec.dropout = None
    cfg_nodec.image_tokens = image_tokens
    # smaller model makes debug output easier to read
    cfg_nodec.n_layers = 2
    cfg_nodec.d_model = 64
    cfg_nodec.num_heads = 4
    if clip_conditioning:
        cfg_nodec.clip_conditioning = True
    if clip_caps:
        cfg_nodec.clip_caps = True
        if clip_cap_count is None:
            clip_cap_count = 2
        cfg_nodec.clip_cap_count = clip_cap_count
    mdl_nodec = ImageModel(**cfg_nodec.__dict__)
    mdl_dec = mdl_nodec.clone(decode=True, flash_attention=False)

    img_toks = jax.random.randint(jax.random.PRNGKey(420), (image_tokens,), 0, 8192)
    if clip_conditioning:
        if clip_caps:
            clip_embedding = jax.random.normal(
                jax.random.PRNGKey(1337), (clip_cap_count, 768)
            )
            clip_embedding = clip_embedding / jnp.linalg.norm(
                clip_embedding, axis=-1, keepdims=True
            )
            max_cos_distance = jnp.full(clip_cap_count, 0.5)
        else:
            clip_embedding = jax.random.normal(jax.random.PRNGKey(1337), (768,))
            clip_embedding = clip_embedding / jnp.linalg.norm(clip_embedding)
            max_cos_distance = jnp.array([])
    else:
        clip_embedding = max_cos_distance = jnp.array([])

    params = mdl_nodec.init(
        jax.random.PRNGKey(69),
        img_toks,
        clip_embedding=clip_embedding,
        max_cos_distance=max_cos_distance,
    )
    # IMPORTANT: use regular __call__ here, not decode_step. The cache needs to be initialized to
    # the full seq_len size.
    params_dec = mdl_dec.init(
        jax.random.PRNGKey(69),
        img_toks,
        clip_embedding=clip_embedding,
        max_cos_distance=max_cos_distance,
    )

    _assert_dicts_equal(params["params"], params_dec["params"], "params")

    logits_all = mdl_nodec.apply(
        params,
        image=img_toks,
        clip_embedding=clip_embedding,
        max_cos_distance=max_cos_distance,
    )

    return (
        mdl_nodec,
        mdl_dec,
        params,
        params_dec["cache"],
        img_toks,
        clip_embedding,
        max_cos_distance,
        logits_all,
    )


def _test_sample_tok_0(
    clip_conditioning: bool, clip_caps: bool, clip_cap_count: Optional[int] = None
) -> None:
    """Test that step-by-step decoding is equivalent to all at once for image token 0."""
    (
        mdl_nodec,
        mdl_dec,
        params,
        cache,
        toks,
        clip_embedding,
        max_cos_distance,
        logits_all,
    ) = _setup_test_sample(clip_conditioning, clip_caps, clip_cap_count)

    params = flax.core.copy(params, {"cache": cache})
    logits_0, cache = mdl_dec.apply(
        params,
        mutable=["cache"],
        method=mdl_dec.decode_init,
        clip_embedding=clip_embedding,
        max_cos_distance=max_cos_distance,
    )
    assert logits_0.shape == (8192,)

    np.testing.assert_allclose(logits_all[0], logits_0, rtol=0, atol=1e-5)


def test_sample_tok_0_no_clip() -> None:
    _test_sample_tok_0(False, False)


def test_sample_tok_0_clip() -> None:
    _test_sample_tok_0(True, False)


def test_sample_tok_0_clip_caps_1() -> None:
    _test_sample_tok_0(True, True, 1)


def test_sample_tok_0_clip_caps_2() -> None:
    _test_sample_tok_0(True, True, 2)


def _test_sample_tok_1(clip_conditioning: bool, clip_caps: bool) -> None:
    """Test that step-by-step decoding is equivalent to all at once for token 1."""
    (
        mdl_nodec,
        mdl_dec,
        params,
        cache,
        toks,
        clip_embedding,
        max_cos_distance,
        logits_all,
    ) = _setup_test_sample(clip_conditioning, clip_caps)

    params = flax.core.copy(params, {"cache": cache})

    _logits_0, cache = mdl_dec.apply(
        params,
        mutable=["cache"],
        method=mdl_dec.decode_init,
        clip_embedding=clip_embedding,
        max_cos_distance=max_cos_distance,
    )
    params = flax.core.copy(params, cache)
    logits_1, _cache = mdl_dec.apply(
        params,
        mutable=["cache"],
        method=mdl_dec.decode_step,
        tok=toks[0],
        idx=jnp.array(0),
    )

    np.testing.assert_allclose(logits_all[1], logits_1, rtol=0, atol=1e-5)


def test_sample_tok_1_no_clip() -> None:
    _test_sample_tok_1(False, False)


def test_sample_tok_1_clip() -> None:
    _test_sample_tok_1(True, False)


def test_sample_tok_1_clip_caps() -> None:
    _test_sample_tok_1(True, True)


def _test_sample_tok_all(
    clip_conditioning: bool, clip_caps: bool, image_tokens: int = 256
) -> None:
    """Test that step-by-step decoding is equivalent to all at once for all tokens."""
    (
        mdl_nodec,
        mdl_dec,
        params,
        cache,
        toks,
        clip_embedding,
        max_cos_distance,
        logits_all,
    ) = _setup_test_sample(clip_conditioning, clip_caps, None, image_tokens)

    decoded_logits = []
    params = flax.core.copy(params, {"cache": cache})

    # compute logits for image tok 0
    logits, new_cache = mdl_dec.apply(
        params,
        mutable=["cache"],
        method=mdl_dec.decode_init,
        clip_embedding=clip_embedding,
        max_cos_distance=max_cos_distance,
    )
    assert logits.shape == (8192,)
    decoded_logits.append(logits)
    params = flax.core.copy(params, new_cache)

    step_j = jax.jit(
        lambda params, i: mdl_dec.apply(
            params,
            mutable=["cache"],
            method=mdl_dec.decode_step,
            tok=toks[i],
            idx=jnp.array(i),
        )
    )

    # compute logits for image toks 1-255 (inputting toks 0-254)
    for i in range(image_tokens - 1):
        logits, new_cache = step_j(params, i)
        assert logits.shape == (8192,)
        decoded_logits.append(logits)
        params = flax.core.copy(params, new_cache)

    decoded_logits = jnp.stack(decoded_logits, axis=0)
    assert decoded_logits.shape == (image_tokens, 8192)
    np.testing.assert_allclose(logits_all, decoded_logits, rtol=0, atol=1e-6)


def test_sample_tok_all_no_clip() -> None:
    _test_sample_tok_all(False, False)


def test_sample_tok_all_clip() -> None:
    _test_sample_tok_all(True, False)


def test_sample_tok_all_clip_caps() -> None:
    _test_sample_tok_all(True, True)


def test_sample_tok_all_clip_caps_1024() -> None:
    # There was a boundary issue with flash attention that broke with sequence lengths that > 1024
    # & not multiples of 1024.
    _test_sample_tok_all(True, True, 1024)


def test_clip_does_anything() -> None:
    """Test that changing the CLIP embedding changes the logits."""
    (
        mdl_nodec,
        mdl_dec,
        params,
        cache,
        toks,
        clip_embedding,
        max_cos_distance,
        logits_all,
    ) = _setup_test_sample(True, False)

    clip_embedding = jnp.zeros_like(clip_embedding)
    logits_all_zero = mdl_nodec.apply(
        params,
        image=toks,
        clip_embedding=clip_embedding,
        max_cos_distance=jnp.array([]),
    )

    assert not jnp.allclose(logits_all, logits_all_zero, rtol=0, atol=1e-3)


def test_clip_caps_do_anything() -> None:
    """Test that changing the CLIP cap size changes the logits."""
    (
        mdl_nodec,
        mdl_dec,
        params,
        cache,
        toks,
        clip_embedding,
        max_cos_distance,
        logits_all,
    ) = _setup_test_sample(True, True)

    logits_full_range = mdl_nodec.apply(
        params,
        image=toks,
        clip_embedding=clip_embedding,
        max_cos_distance=jnp.array([2.0, 0.85]),
    )

    assert not jnp.allclose(logits_all, logits_full_range, rtol=0, atol=1e-3)


@partial(jax.jit, static_argnums=(0,), inline=True)
def sample(
    mdl: ImageModel,
    params: dict[str, Any],
    clip_embedding: jax.Array,
    max_cos_distance: jax.Array,
    rng: jax.Array,
    top_p: float = 0.95,
) -> jax.Array:
    """Sample a single image from the model. Returns an array of codes to be passed to the
    LDM decoder."""
    if mdl.clip_conditioning and mdl.clip_caps:
        assert clip_embedding.shape == (mdl.clip_cap_count, 768)
        assert max_cos_distance.shape == (mdl.clip_cap_count,)
    elif mdl.clip_conditioning and not mdl.clip_caps:
        assert clip_embedding.shape == (768,)
        assert max_cos_distance.shape == (0,)
    else:
        assert clip_embedding.shape == max_cos_distance.shape == (0,)

    # Flash attention doesn't work with Flax's fast decoding. Something to do with how masks are
    # handled. Would be nice to fix it, but for now we just use the slower attention when sampling.
    mdl_decode = mdl.clone(decode=True, flash_attention=False, dropout=0.0)
    params_fake = mdl_decode.init(
        jax.random.PRNGKey(0),
        image=jnp.zeros((mdl.image_tokens,), dtype=jnp.int32),
        clip_embedding=clip_embedding,
        max_cos_distance=max_cos_distance,
    )
    params = flax.core.copy(params, {"cache": params_fake["cache"]})
    del params_fake

    # This needs to be outside the linen module because the fori_loop combinator doesn't work
    # inside them.
    def loop_iter(
        i: int, acc: Tuple[jax.Array, jax.Array]
    ) -> Tuple[jax.Array, jax.Array, dict[str, Any]]:
        image_toks, rng, params = acc
        logits, new_cache = mdl_decode.apply(
            params,
            mutable=["cache"],
            method=mdl_decode.decode_step,
            tok=image_toks[i],
            idx=i,
        )
        assert logits.shape == (8192,)
        params = flax.core.copy(params, new_cache)
        filtered_logits = _filter_top_p(logits, top_p)
        rng_sample, rng_loop = jax.random.split(rng, 2)
        tok = jax.random.categorical(rng_sample, filtered_logits)
        image_toks = image_toks.at[i + 1].set(tok)
        return (image_toks, rng_loop, params)

    rng0, rng_loop = jax.random.split(rng, 2)
    logits_0, cache = mdl_decode.apply(
        params,
        mutable=["cache"],
        method=mdl_decode.decode_init,
        clip_embedding=clip_embedding,
        max_cos_distance=max_cos_distance,
    )
    assert logits_0.shape == (8192,)
    filtered_logits_0 = _filter_top_p(logits_0, top_p)
    tok_0 = jax.random.categorical(rng0, filtered_logits_0)

    params = flax.core.copy(params, cache)

    image_toks = jnp.zeros((mdl.image_tokens,), dtype=jnp.int32).at[0].set(tok_0)
    image_toks, _, _ = jax.lax.fori_loop(  # type: ignore[no-untyped-call]
        0,
        mdl.image_tokens - 1,
        loop_iter,
        (image_toks, rng_loop, params),
    )
    return image_toks  # type: ignore[no-any-return]


def _filter_top_p(logits: jax.Array, top_p: float) -> jax.Array:
    """Filter an array of logits to include the smallest subset of possibilities that has
    proability mass at least p i.e. top p/nucleus sampling. Returns the filtered array.
    """
    probs = jax.nn.softmax(logits)
    sorted_probs, sorted_indices = (
        jnp.sort(probs)[::-1],
        jnp.argsort(probs)[::-1],
    )
    cumulative_probs = jnp.cumsum(sorted_probs)
    # collect the minimal set of possibilites with probability <= top_p
    mask = cumulative_probs <= top_p
    # Find the index of the first possibility that has cumulative probability >= top_p
    # this might be the last element we found above or might be the one after it.
    # we could do only the argmax and set the mask with a range but that's not JIT-able so we do
    # this.
    last_idx = jnp.argmax(cumulative_probs >= top_p)
    mask = mask.at[last_idx].set(True)

    # permute the mask back to the original order
    mask = mask[sorted_indices.argsort()]

    filtered_logits = jnp.where(mask, logits, -np.inf)
    return filtered_logits


def test_filter_top_p_10() -> None:
    """Test that filter_top_p is the identity function when top_p = 1.0."""
    logits = jnp.arange(10)
    filtered_logits = _filter_top_p(logits, 1.0)
    assert jnp.allclose(
        filtered_logits, logits
    ), "filter_top_p doesn't match the identity function when top_p = 1.0"


def test_filter_top_p_05() -> None:
    """Test that filter_top_p removes low-probability elements when top_p = 0.5."""
    probabilities = jnp.array([0.35, 0.35, 0.1, 0.1, 0.1])
    assert jnp.isclose(jnp.sum(probabilities), 1.0)
    logits = jnp.log(probabilities)
    filtered_logits = _filter_top_p(logits, 0.5)
    np.testing.assert_allclose(
        np.array(jax.nn.softmax(filtered_logits)), np.array([0.5, 0.5, 0, 0, 0])
    )


def test_filter_top_p_out_of_order() -> None:
    """Test that filter_top_p removes low-probability elements when inputs do not start sorted."""
    probabilities = np.repeat(1000.0, 7)
    big_indices = np.array([3, 5])
    medium_indices = np.array([2, 4])
    small_indices = np.array([0, 1, 6])
    probabilities[big_indices] = 0.25
    probabilities[medium_indices] = 0.2
    probabilities[small_indices] = 0.1 / 3.0
    np.testing.assert_allclose(np.sum(probabilities), 1.0)

    logits = jnp.log(probabilities)
    filtered_logits = _filter_top_p(logits, 0.75)
    filtered_probabilities = np.array(jax.nn.softmax(filtered_logits))

    np.testing.assert_allclose(filtered_probabilities[small_indices], 0.0)
    np.testing.assert_allclose(filtered_probabilities[medium_indices], 0.2 / 0.9)
    np.testing.assert_allclose(filtered_probabilities[big_indices], 0.25 / 0.9)


class TransformerLayer(nn.Module):
    """A single transformer layer."""

    d_model: int
    num_heads: int
    ff_dim: int
    dropout: Optional[float]
    use_biases: bool
    activations_dtype: jnp.dtype
    activation_function: Callable[[jax.Array], jax.Array]
    kernel_init: Callable[..., jnp.ndarray]
    decode: bool
    flash_attention: bool

    def setup(self) -> None:
        if self.flash_attention:
            # Use fast flash attention implementation
            def attn_function(
                q,
                k,
                v,
                bias=None,
                mask=None,
                broadcast_dropout=True,
                dropout_rng=None,
                dropout_rate=0.0,
                deterministic=False,
                dtype=None,
                precision=None,
            ):
                assert len(q.shape) == 3, "batch dimensions not implemented"
                assert q.shape[1] == k.shape[1] == v.shape[1]
                assert q.shape[2] == k.shape[2] == v.shape[2]
                assert k.shape[0] == v.shape[0]

                rearrange_qkv = lambda x: rearrange(
                    x, "seq_len heads head_dim -> 1 heads seq_len head_dim"
                )
                q, k, v = map(rearrange_qkv, (q, k, v))

                assert bias == None, "attention bias not implemented"
                assert (
                    mask == None
                ), "attention mask is redundant with causal_flash_attention"
                assert dropout_rate == 0.0, "attention dropout not implemented"

                try:
                    res = causal_flash_attention(q, k, v)
                except TypeError as e:
                    if "cannot reshape array of shape" in str(e):
                        raise ValueError(
                            (
                                "Got an exception from causal_flash_attention: {}. You may have "
                                "run into its bug with sequence lengths that are not a multiple of "
                                "the chunk size."
                            ).format(e)
                        )
                if dtype != None:
                    assert res.dtype == dtype

                return rearrange(
                    res, "1 heads seq_len head_dim -> seq_len heads head_dim"
                )

        else:
            attn_function = nn.attention.dot_product_attention
        self.mha = nn.SelfAttention(
            num_heads=self.num_heads,
            qkv_features=self.d_model,
            # dropout in the attention matrix was introduced in
            # https://arxiv.org/abs/1907.11065, it's *not* the normal thing
            # from Attention is All You Need.
            dropout_rate=0,
            deterministic=False,
            use_bias=self.use_biases,
            dtype=self.activations_dtype,
            kernel_init=self.kernel_init,
            decode=self.decode,
            attention_fn=attn_function,
        )
        self.layer_norm_1 = nn.LayerNorm(dtype=self.activations_dtype)
        self.linear_1 = nn.Dense(
            features=self.ff_dim,
            use_bias=self.use_biases,
            kernel_init=self.kernel_init,
            dtype=self.activations_dtype,
        )
        self.linear_2 = nn.Dense(
            features=self.d_model,
            use_bias=self.use_biases,
            kernel_init=self.kernel_init,
            dtype=self.activations_dtype,
        )
        self.layer_norm_2 = nn.LayerNorm(dtype=self.activations_dtype)
        if self.dropout is not None:
            self.dropout_layer = nn.Dropout(self.dropout, deterministic=False)
        else:
            self.dropout_layer = nn.Dropout(rate=0, deterministic=True)

    def __call__(self, embeds: jax.Array, _) -> jax.Array:
        if self.flash_attention:
            mask = None
        else:
            mask = jnp.tril(
                jnp.ones((self.num_heads, embeds.shape[0], embeds.shape[0]))
            )
        out_block_1 = self.layer_norm_1(self.mha(embeds, mask=mask))
        in_block_2: jax.Array = embeds + self.dropout_layer(out_block_1)
        out_block_2: jax.Array = self.layer_norm_2(
            self.dropout_layer(
                self.linear_2(self.activation_function(self.linear_1(in_block_2)))  # type: ignore[attr-defined]
            )
        )
        return in_block_2 + out_block_2, None


def test_flash_attention_equals_standard() -> None:
    """Test that flash attention gives the same results as Flax's standard attention."""
    mdl_std = TransformerLayer(
        d_model=768,
        num_heads=12,
        ff_dim=3072,
        dropout=None,
        use_biases=False,
        activations_dtype=jnp.float32,
        activation_function=jax.nn.relu,
        kernel_init=jax.nn.initializers.xavier_uniform(),
        decode=False,
        flash_attention=False,
    )

    input_shape = (64, 768)
    input_vals = jax.random.normal(jax.random.PRNGKey(0), input_shape)

    params = mdl_std.init(
        jax.random.PRNGKey(1), jnp.ones(input_shape, dtype=jnp.float32), None
    )

    out_std, _ = mdl_std.apply(params, input_vals, None)

    mdl_flash = mdl_std.clone(flash_attention=True)
    out_flash, _ = mdl_flash.apply(params, input_vals, None)

    np.testing.assert_allclose(out_std, out_flash, atol=3e-6, rtol=0)


def loss(
    model: ImageModel,
    params: dict[str, Any],
    dropout_rng: jax.Array,
    ex_img: jax.Array,
    ex_clip: jax.Array,
    ex_max_cos_distance: jax.Array,
) -> jax.Array:
    """Compute the cross-entropy loss for a single example."""
    assert ex_img.shape == (
        model.image_tokens,
    ), f"ex_img.shape: {ex_img.shape}, expected: {(model.image_tokens,)}"
    if model.clip_conditioning and not model.clip_caps:
        assert ex_clip.shape == (768,)
        assert ex_max_cos_distance.shape == (0,)
    elif model.clip_conditioning and model.clip_caps:
        assert ex_clip.shape == (model.clip_cap_count, 768)
        assert ex_max_cos_distance.shape == (model.clip_cap_count,)
    else:
        assert ex_clip.shape == ex_max_cos_distance.shape == (0,)
    logits: jax.Array = model.apply(
        params,
        rngs={"dropout": dropout_rng},
        image=ex_img,
        clip_embedding=ex_clip,
        max_cos_distance=ex_max_cos_distance,
    )
    return optax.softmax_cross_entropy(logits, jax.nn.one_hot(ex_img, 8192))  # type: ignore[no-any-return]


def loss_batch(
    model: ImageModel,
    params: dict[str, Any],
    dropout_rng: jax.Array,
    batch_imgs: jax.Array,
    batch_clips: jax.Array,
    batch_max_cos_distances: jax.Array,
) -> jax.Array:
    """Compute the cross-entropy loss for a batch of examples."""
    assert (
        batch_imgs.shape[0] == batch_clips.shape[0] == batch_max_cos_distances.shape[0]
    )
    return jnp.mean(
        jax.vmap(loss, in_axes=(None, None, 0, 0, 0, 0))(
            model,
            params,
            jax.random.split(dropout_rng, batch_imgs.shape[0]),
            batch_imgs,
            batch_clips,
            batch_max_cos_distances,
        )
    )


# Parameters taken from GPT-1, except seq_len is 256 instead of 1024
gpt_1_config = ModelConfig(
    d_model=768,
    num_heads=12,
    ff_dim=3072,
    dropout=0.1,
    n_layers=12,
    image_tokens=256,
    use_biases=True,
    activation_function=jax.nn.relu,
    clip_conditioning=False,
)


def test_cap_train() -> None:
    """Test the model can memorize some image/clip pairs."""
    mdl_cfg = copy(gpt_1_config)
    mdl_cfg.clip_conditioning = True
    mdl_cfg.clip_caps = True
    mdl_cfg.clip_cap_count = 9
    mdl_cfg.dropout = None
    # This may or may not actually need a model this big, I'm tired of fucking with it.
    mdl_cfg.d_model = 1024
    mdl_cfg.num_heads = 16
    mdl_cfg.n_layers = 24

    n_imgs = 8

    mdl = ImageModel(**mdl_cfg.__dict__)

    (
        img_rng,
        clip_rng,
        max_cos_distance_rng,
        params_rng,
        train_rng,
        test_rng,
    ) = jax.random.split(jax.random.PRNGKey(0), 6)

    imgs = jax.random.randint(img_rng, (n_imgs, mdl.image_tokens), 0, 8192)
    clips = jax.random.normal(clip_rng, (n_imgs, mdl.clip_cap_count, 768))
    clips = clips / jnp.linalg.norm(clips, axis=-1, keepdims=True)
    max_cos_distances = jax.random.uniform(
        max_cos_distance_rng, shape=(n_imgs, mdl.clip_cap_count), minval=0.0, maxval=2.0
    )

    params = mdl.init(
        {"params": params_rng, "dropout": jax.random.PRNGKey(0)},
        image=jnp.zeros((mdl.image_tokens,), dtype=jnp.int32),
        clip_embedding=jnp.zeros((mdl.clip_cap_count, 768), dtype=jnp.float32),
        max_cos_distance=jnp.zeros((mdl.clip_cap_count,), dtype=jnp.float32),
    )

    loss_grad_fn = jax.value_and_grad(loss_batch, argnums=1)

    steps = 500
    adam = optax.adam(learning_rate=triangle_schedule(3e-5, steps))
    opt = optax.chain(optax.clip_by_global_norm(0.25), adam)
    opt_state = opt.init(params)

    def opt_step(params, opt_state, rng):
        dropout_rng, rng2 = jax.random.split(rng, 2)
        loss, grads = loss_grad_fn(
            mdl,
            params,
            dropout_rng,
            batch_imgs=imgs,
            batch_clips=clips,
            batch_max_cos_distances=max_cos_distances,
        )
        updates, opt_state = opt.update(grads, opt_state)
        new_params = optax.apply_updates(params, updates)
        norm = optax.global_norm(grads)
        return new_params, opt_state, rng2, loss, norm

    opt_step = jax.jit(opt_step, donate_argnums=(0, 1, 2))

    for i in trange(steps):
        params, opt_state, train_rng, loss, norm = opt_step(
            params, opt_state, train_rng
        )
        tqdm.write(f"iter {i:04d} loss: {loss:0.4f} grad norm: {norm:7.2f}")

    sampled_imgs = []
    for i in trange(n_imgs):
        sample_rng, test_rng = jax.random.split(test_rng, 2)

        # This only tests the ability to memorize training examples, and not even ability to
        # generalize to different sets of caps for the same image.

        toks = sample(
            mdl,
            params,
            clips[i],
            max_cos_distances[i],
            sample_rng,
            top_p=0.25,
        )
        sampled_imgs.append(toks)

    sampled_imgs = jnp.stack(sampled_imgs, axis=0)

    # Test that sampled images are the training images
    correct_imgs = [0] * n_imgs
    for img in sampled_imgs:
        for i, train_img in enumerate(imgs):
            if jnp.all(img == train_img):
                correct_imgs[i] += 1
                break
    correct_img_cnt = sum(correct_imgs)
    print(f"correct_imgs: {correct_imgs}, count {correct_img_cnt}")
    assert correct_img_cnt == n_imgs

    # Test that sampled images match the CLIP conditioning
    correct_conds = [False] * n_imgs
    for i in range(n_imgs):
        correct_conds[i] = jnp.array_equal(imgs[i], sampled_imgs[i])
    correct_conds = np.array(correct_conds)
    print(f"correct_conds: {correct_conds}")
    assert all(correct_conds)


def train_loop_simple(
    data: jax.Array, mdl: ImageModel, iters: int
) -> Tuple[jax.Array, dict[str, Any]]:
    """Train the model repeatedly on a single batch for testing."""
    assert mdl.clip_conditioning is False
    params = mdl.init(
        rngs={"dropout": jax.random.PRNGKey(0), "params": jax.random.PRNGKey(1)},
        image=jnp.zeros((gpt_1_config.image_tokens,), dtype=jnp.int32),
        clip_embedding=jnp.zeros((0,), dtype=jnp.float32),
        max_cos_distance=jnp.array([]),
    )
    opt = optax.adam(learning_rate=3e-5)
    opt_state = opt.init(params)
    loss_grad_fn = jax.value_and_grad(loss_batch, argnums=1)

    def opt_step(
        params: dict[str, Any],
        opt_state: Any,
        rng: jax.Array,
        batch_imgs: jax.Array,
    ) -> Tuple[dict[str, Any], Any, jax.Array, jax.Array]:
        dropout_rng, caps_rng, rng2 = jax.random.split(rng, 3)
        loss, grads = loss_grad_fn(
            mdl,
            params,
            dropout_rng,
            batch_imgs=batch_imgs,
            batch_clips=jnp.zeros((batch_imgs.shape[0], 0), dtype=jnp.float32),
            batch_max_cos_distances=jnp.zeros(
                (batch_imgs.shape[0], 0), dtype=jnp.float32
            ),
        )
        updates, opt_state = opt.update(grads, opt_state)
        new_params: dict[str, Any] = optax.apply_updates(params, updates)
        return new_params, opt_state, rng2, loss

    opt_step = jax.jit(opt_step, donate_argnums=(0, 1, 2))

    train_rng = jax.random.PRNGKey(0)
    for i in range(iters):
        params, opt_state, train_rng, loss = opt_step(  # type:ignore[misc]
            params, opt_state, train_rng, data
        )
        print(f"iter {i} loss: {loss}")
    return loss, params  # type:ignore[return-value]


def test_learn_zeros() -> None:
    """Test whether the model can learn to predict all zeros."""
    mdl = ImageModel(**gpt_1_config.__dict__)
    data = jnp.zeros((16, gpt_1_config.image_tokens), dtype=jnp.int32)
    loss, params = train_loop_simple(data, mdl, 10)
    assert loss < 1e-10

    sampled_arr = sample(
        mdl,
        params,
        jnp.zeros((0,), dtype=jnp.float32),
        jnp.array([], dtype=jnp.float32),
        jax.random.PRNGKey(0),
    )
    assert jnp.all(sampled_arr == 0)


def test_learn_ranges() -> None:
    """Test whether the model can learn to predict a range of integers."""
    mdl = ImageModel(**gpt_1_config.__dict__)
    data = jnp.arange(16 * gpt_1_config.image_tokens).reshape(
        (16, gpt_1_config.image_tokens)
    )
    # It's annoying how many iterations we need to get to minimal loss on such a trivial dataset
    loss, params = train_loop_simple(data, mdl, 1000)
    assert loss < 0.6
    empty_arr = jnp.zeros((0,), dtype=jnp.float32)
    sample_jv = jax.jit(
        lambda params, rng: jax.vmap(
            lambda rng: sample(mdl, params, empty_arr, empty_arr, rng, top_p=0.98)
        )(rng)
    )
    print("Generating samples...")
    total_samples = 256
    samples_per_batch = 64
    assert total_samples % samples_per_batch == 0
    sample_batches = []
    rng = jax.random.PRNGKey(0)
    for _ in trange(total_samples // samples_per_batch):
        rng, rng2 = jax.random.split(rng)
        sample_batches.append(
            sample_jv(params, jax.random.split(rng2, samples_per_batch))
        )
    sampled_arr: jax.Array = jnp.concatenate(sample_batches, axis=0)
    print(f"Generated samples, shape: {sampled_arr.shape}")
    counts = dict([(i, 0) for i in range(16)])
    for i, s in enumerate(sampled_arr):
        assert s[0] % 256 == 0, f"sample {i} starts with {s[0]}"
        np.testing.assert_equal(np.array(s), np.array(s[0] + np.arange(256)))
        counts[int(s[0] // 256)] += 1
    print(f"counts: {counts}")
    assert all([c > 0 for c in counts.values()])


def test_clip_caps_overfit():
    """Using a collection of ~100k, cap-image pair training examples, train a model and check test
    loss, then sample and check the generated samples. The training data is 100k pairs generated
    from 100 images (drawn with replacement). This tests the model's ability to memorize a small
    set of image encodings and learn to sample based on their CLIP encodings being within caps.
    """

    params_rng, sample_rng = jax.random.split(jax.random.PRNGKey(42), 2)
    # run get-test-data.sh to download this
    dset_all = load_pq_to_infinidata(Path(__file__).parent.parent / "test-images/examples-100.pq").shuffle(
        seed=420_69
    )
    dset_train = dset_all.new_view(slice(None, -16))
    dset_test = dset_all.new_view(slice(-16, None))

    mdl_cfg = copy(gpt_1_config)
    mdl_cfg.clip_conditioning = True
    mdl_cfg.clip_caps = True
    mdl_cfg.clip_cap_count = 1
    mdl_cfg.dropout = None
    mdl_cfg.d_model = 1024
    mdl_cfg.num_heads = 16
    mdl_cfg.n_layers = 24

    mdl = ImageModel(**mdl_cfg.__dict__)

    params = _train_clip_caps_overfit(dset_train, mdl, params_rng)
    _test_clip_caps_overfit_test_loss(dset_test, mdl, params)
    _test_clip_caps_overfit_samples(dset_test, mdl, params, sample_rng)


def _train_clip_caps_overfit(dset_train, mdl, rng):
    """Train a test model"""
    params_rng, dropout_rng = jax.random.split(rng, 2)

    params = mdl.init(
        {"params": params_rng},
        image=jnp.zeros((mdl.image_tokens,), dtype=jnp.int32),
        clip_embedding=jnp.zeros((mdl.clip_cap_count, 768), dtype=jnp.float32),
        max_cos_distance=jnp.zeros((mdl.clip_cap_count,), dtype=jnp.float32),
    )

    loss_grad_fn = jax.value_and_grad(loss_batch, argnums=1)

    steps = 2_000
    batch_size = 36

    adam = optax.adam(learning_rate=triangle_schedule(1e-4, steps))
    opt = optax.chain(optax.clip_by_global_norm(0.25), adam)
    opt_state = opt.init(params)

    def opt_step(params, opt_state, rng, images, clips, max_cos_distances):
        dropout_rng, rng2 = jax.random.split(rng, 2)
        loss, grads = loss_grad_fn(
            mdl,
            params,
            dropout_rng,
            batch_imgs=images,
            batch_clips=clips,
            batch_max_cos_distances=max_cos_distances,
        )
        updates, opt_state = opt.update(grads, opt_state)
        new_params = optax.apply_updates(params, updates)
        norm = optax.global_norm(grads)
        return new_params, opt_state, rng2, loss, norm

    opt_step = jax.jit(opt_step, donate_argnums=(0, 1, 2))

    with tqdm(total=steps) as pbar:
        while pbar.n < steps:
            dset_shuf = dset_train.shuffle()
            for batch in dset_shuf.batch_iter(batch_size, drop_last_batch=True):
                images, cap_centers, max_cos_distances = (
                    batch["encoded_img"],
                    rearrange(batch["cap_center"], "b c -> b 1 c"),
                    rearrange(batch["cap_max_cos_distance"], "b -> b 1"),
                )
                assert (
                    images.shape[0]
                    == cap_centers.shape[0]
                    == max_cos_distances.shape[0]
                )
                params, opt_state, dropout_rng, loss, norm = opt_step(
                    params,
                    opt_state,
                    dropout_rng,
                    images,
                    cap_centers,
                    max_cos_distances,
                )
                loss, norm = np.asarray(loss), np.asarray(norm)
                pbar.update(1)
                pbar.set_postfix({"loss": loss, "grad norm": norm})
                if pbar.n % 100 == 0:
                    tqdm.write(
                        f"iter {pbar.n:04d} loss: {loss:0.4f} grad norm: {norm:05.2f}"
                    )
                if pbar.n >= steps:
                    break
    return params


def _test_clip_caps_overfit_test_loss(dset_test, mdl, params):
    """Compute test loss and assert it's low enough"""
    examples = dset_test[:]
    cap_centers = rearrange(examples["cap_center"], "b c -> b 1 c")
    max_cos_distances = rearrange(examples["cap_max_cos_distance"], "b -> b 1")
    imgs = examples["encoded_img"]

    computed_loss = loss_batch(
        mdl, params, jax.random.PRNGKey(0), imgs, cap_centers, max_cos_distances
    )
    print(f"test loss: {computed_loss}")
    assert computed_loss < 0.05


def _test_clip_caps_overfit_samples(dset_test, mdl, params, rng):
    """Sample from the model and check it returns images with embeddings inside the test caps."""
    pt_rng, sample_rng = jax.random.split(rng, 2)

    examples = dset_test[:]
    n_samples = len(examples["encoded_img"])
    embeddings = examples["clip_embedding"]

    query_pts = jax.vmap(random_pt_with_cosine_similarity, in_axes=(0, 0, None))(
        jax.random.split(pt_rng, n_samples), embeddings, 0.8
    )
    query_max_cos_distances = jnp.full((n_samples,), 0.25)

    matches = np.zeros((n_samples,), dtype=np.bool_)
    for i in trange(n_samples):
        sample_rng, sub_rng = jax.random.split(sample_rng, 2)
        toks = sample(
            mdl,
            params,
            query_pts[i : i + 1],
            query_max_cos_distances[i : i + 1],
            sub_rng,
            top_p=0.05,
        )
        matches[i] = np.array_equal(toks, examples["encoded_img"][i])

    print(f"Found {matches.sum()} matches out of {n_samples} samples")
    assert matches.sum() >= round(n_samples * 0.9)
